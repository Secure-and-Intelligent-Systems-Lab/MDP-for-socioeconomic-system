{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Including Libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from scipy.stats import genpareto\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,gamma):        \n",
    "        self.state_size = 2\n",
    "        self.action_size = 5\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        self.gamma = gamma   # discount rate\n",
    "        self.epsilon = 1  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9999\n",
    "        self.learning_rate = 0.001\n",
    "        self.X_batch=[]\n",
    "        self.Y_batch=[]\n",
    "        self.target_weights=[]\n",
    "        self.weights=[]\n",
    "        self.cost_array=[]\n",
    "        self.action_array=[]\n",
    "        \n",
    "        self.tau = 1 #target weights update rate        \n",
    "              \n",
    "        self.model        = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, input_dim=self.state_size, activation='linear'))\n",
    "        model.add(Dense(24, activation='linear'))\n",
    "        model.add(Dense(12, activation='linear'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=tf.keras.losses.Huber(),\n",
    "                      optimizer=optimizers.RMSprop(lr=self.learning_rate, clipnorm=1))   \n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, cost, next_state):\n",
    "        self.memory.append((state, action, cost, next_state))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(5)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmin(act_values)  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        state_array = np.vstack([x[0] for x in minibatch])\n",
    "        self.action_array = np.array([x[1] for x in minibatch])\n",
    "        self.cost_array = np.array([x[2] for x in minibatch])\n",
    "        next_state_array = np.vstack([x[3] for x in minibatch])\n",
    "        self.X_batch = state_array\n",
    "        self.Y_batch = self.model.predict(state_array)\n",
    "\n",
    "        Q_target = self.cost_array +self.gamma * np.amin(self.target_model.predict(next_state_array), axis=1) \n",
    "        self.Y_batch[np.arange(len(self.X_batch)), self.action_array] = Q_target\n",
    "\n",
    "        self.model.fit(self.X_batch, self.Y_batch, epochs=1,verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def target_train(self):\n",
    "        self.weights = self.model.get_weights()\n",
    "        self.target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(self.target_weights)):\n",
    "            self.target_weights[i] = self.weights[i] * self.tau + self.target_weights[i] * (1 - self.tau)\n",
    "        self.target_model.set_weights(self.target_weights)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOAA sea level rise predictions\n",
    "#input 'scale_parameter_gammadstrb'\n",
    "B= 0.5;\n",
    "\n",
    "#input 'shape_parameter_gammadstrb'\n",
    "A= np.zeros((3,100))\n",
    "A[0,:]=np.arange(11.2,12.388,0.012);# intermediate low rise case\n",
    "A[1,:]=np.arange(13,35,0.22);# intermediate high rise case\n",
    "A[2,:]= np.arange(14.6,88.6,0.74);# high rise case\n",
    "\n",
    "\n",
    "# generalized pareto    \n",
    "power_l=0.9;\n",
    "power_s= 0.8;  \n",
    "eta=250;\n",
    "k= -0.1;\n",
    "theta= 14;\n",
    "\n",
    "#parameters for cost definition\n",
    "beta=14 ; #coefficient of residents' investment decision y_n, residents', contribution of 14M $/yearly, 1M $ taken as unit\n",
    "alpha=300; # 25m $ is the coefficient for investment cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a_g= 0.9; ## Government's cooperation index\n",
    "a_r= 0.9; ## Residents' cooperation index\n",
    "a=2; ## SLR scenarios; a=0,1, and 2, respectively for intermediate low, intermediate, and high sea level rise projections. \n",
    "s_norm=400,# normalozing s\n",
    "l_norm=[580, 1190,2590];# normalozing s\n",
    "scene= ['low','inter','high']\n",
    "\n",
    "EPISODES= 1000000;\n",
    "Final_Episodes=100; ## episodes used for final cost evaluation\n",
    "\n",
    "years=100; #future to be considered in years \n",
    "\n",
    "agent = DQNAgent(a_g);\n",
    "batch_size=1000;\n",
    "state= np.zeros((100,2));\n",
    "next_state= np.zeros((100,2));\n",
    "convergence=[];\n",
    "w00= np.zeros((2,12));\n",
    "w01= np.zeros((12,24));\n",
    "w02= np.zeros((24,12));\n",
    "w03= np.zeros((12,5));\n",
    "b00= np.zeros(12);\n",
    "b01= np.zeros(24);\n",
    "b02= np.zeros(12);\n",
    "b03= np.zeros(5);\n",
    "\n",
    "total_cost= 0; ## average cost over all episodes\n",
    "final_cost= 0; ## average cost for last 10000 episodes(convergence expected by then)\n",
    "\n",
    "for e in range(EPISODES):\n",
    "\tl= np.zeros(101);\n",
    "\ts= np.zeros(101);\n",
    "\tl[0]=100; # initial relative sea level\n",
    "\ts[0]=50; # initial infrastructure state  \n",
    "\tr=0; # sea level rise each year\n",
    "\tq= np.zeros(101);  ## yearly residents' decision score          \n",
    "\tp= np.zeros(101);  ## yearly sigmoid input for residents' decision           \n",
    "\tres= np.zeros(101); ## yearly residents' binary decision \n",
    "\tx= np.zeros(100);   ##  yearly govenment's decision         \n",
    "\tz= np.zeros(100); ## yearly cost from nature\n",
    "            \n",
    "\tfor y in range(years):\n",
    "\n",
    "\t\tz[y]=genpareto.rvs(k, loc=theta, scale=eta* np.power(l[y],power_l)/np.power(s[y],power_s)); # genpareto, shape,k=-0.001,location,theta=0; scale,sigma         \n",
    "               \n",
    "\t\tstate[y,0]= (l[y]-l[0])/l_norm[a];             \n",
    "\t\tstate[y,1]= (s[y]-s[0])/s_norm;             \n",
    "                \n",
    "\t\taction = agent.act(np.reshape(state[y],(1,2)));\n",
    "\t\tx[y]= action; \n",
    "\t\tnext_state[y,0]= (l[y]-l[0]+A[a,y]/2)/l_norm[a];            \n",
    "\t\tnext_state[y,1]= (s[y]-s[0]+action)/s_norm;  \n",
    "\t\tcost= alpha*x[y]-beta*res[y]+z[y];  \n",
    "\t\ttotal_cost=total_cost+ cost;\n",
    "\t\tif e> np.absolute(EPISODES-Final_Episodes-1):\n",
    "\t\t\tfinal_cost=final_cost+ cost;                        \n",
    "\t\tagent.remember(state[y], action, cost, next_state[y]) \n",
    "\t\tif len(agent.memory) > batch_size:\n",
    "\t\t\tw0= agent.model.layers[0].get_weights()[0];\n",
    "\t\t\tw1= agent.model.layers[1].get_weights()[0];                        \n",
    "\t\t\tw2= agent.model.layers[2].get_weights()[0];                        \n",
    "\t\t\tw3= agent.model.layers[3].get_weights()[0];  \n",
    "\t\t\tb0= agent.model.layers[0].get_weights()[1];\n",
    "\t\t\tb1= agent.model.layers[1].get_weights()[1];                        \n",
    "\t\t\tb2= agent.model.layers[2].get_weights()[1];                        \n",
    "\t\t\tb3= agent.model.layers[3].get_weights()[1];                        \n",
    "\t\t\tconv=np.sum(np.power((w0-w00),2))+np.sum(np.power((w1-w01),2))+np.sum(np.power((w2-w02),2))+np.sum(np.power((w3-w03),2))+np.sum(np.power((b0-b00),2))+np.sum(np.power((b1-b01),2))+np.sum(np.power((b2-b02),2))+np.sum(np.power((b3-b03),2));\n",
    "\t\t\tconvergence=  np.append(convergence,conv);\n",
    "\t\t\tw00=w0;\n",
    "\t\t\tw01=w1;\n",
    "\t\t\tw02=w2;\n",
    "\t\t\tw03=w3;\n",
    "\t\t\tb00=b0;\n",
    "\t\t\tb01=b1;\n",
    "\t\t\tb02=b2;\n",
    "\t\t\tb03=b3;                        \n",
    "\t\t\tagent.replay(batch_size); \n",
    "\t\t\tif y % 30 == 1:       \n",
    "\t\t\t\tagent.target_train() \n",
    "                \n",
    "\t\tq[y+1]=a_r*(q[y]+ action/4 * z[y]); ## dividing the action by 4 because action can be o~3\n",
    "\t\tp[y+1]= 1/(1 + np.exp(-(q[y+1]-5)));\n",
    "\t\tres[y+1]= np.random.binomial(1, p[y+1]);  \n",
    "\n",
    "\t\tr= np.random.gamma(A[a,y],B);                                         \n",
    "\t\tl[y+1]= l[y]+r;                \n",
    "\t\ts[y+1]= s[y]+action; \n",
    "        \n",
    "            \n",
    "total_cost=total_cost/EPISODES;\n",
    "final_cost=final_cost/Final_Episodes;\n",
    "print(\"Total cost: {}, final cost: {}\" .format(total_cost,final_cost))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
