{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Including Libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from scipy.stats import genpareto\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,gamma):        \n",
    "        self.state_size = 2\n",
    "        self.action_size = 5\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        self.gamma = gamma   # discount rate\n",
    "        self.epsilon = 1  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9999\n",
    "        self.learning_rate = 0.001\n",
    "        self.X_batch=[]\n",
    "        self.Y_batch=[]\n",
    "        self.target_weights=[]\n",
    "        self.weights=[]\n",
    "        self.cost_array=[]\n",
    "        self.action_array=[]\n",
    "        \n",
    "        self.tau = 1 #target weights update rate        \n",
    "              \n",
    "        self.model        = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, input_dim=self.state_size, activation='linear'))\n",
    "        model.add(Dense(24, activation='linear'))\n",
    "        model.add(Dense(12, activation='linear'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=tf.keras.losses.Huber(),\n",
    "                      optimizer=optimizers.RMSprop(lr=self.learning_rate, clipnorm=1))   \n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, cost, next_state):\n",
    "        self.memory.append((state, action, cost, next_state))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(5)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmin(act_values)  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        state_array = np.vstack([x[0] for x in minibatch])\n",
    "        self.action_array = np.array([x[1] for x in minibatch])\n",
    "        self.cost_array = np.array([x[2] for x in minibatch])\n",
    "        next_state_array = np.vstack([x[3] for x in minibatch])\n",
    "        self.X_batch = state_array\n",
    "        self.Y_batch = self.model.predict(state_array)\n",
    "\n",
    "        Q_target = self.cost_array +self.gamma * np.amin(self.target_model.predict(next_state_array), axis=1) \n",
    "        self.Y_batch[np.arange(len(self.X_batch)), self.action_array] = Q_target\n",
    "\n",
    "        self.model.fit(self.X_batch, self.Y_batch, epochs=1,verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def target_train(self):\n",
    "        self.weights = self.model.get_weights()\n",
    "        self.target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(self.target_weights)):\n",
    "            self.target_weights[i] = self.weights[i] * self.tau + self.target_weights[i] * (1 - self.tau)\n",
    "        self.target_model.set_weights(self.target_weights)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOAA sea level rise predictions\n",
    "#input 'scale_parameter_gammadstrb'\n",
    "B= 0.5;\n",
    "\n",
    "#input 'shape_parameter_gammadstrb'\n",
    "A= np.zeros((3,100))\n",
    "A[0,:]=np.arange(11.2,12.388,0.012);# intermediate low rise case\n",
    "A[1,:]=np.arange(13,35,0.22);# intermediate high rise case\n",
    "A[2,:]= np.arange(14.6,88.6,0.74);# high rise case\n",
    "\n",
    "\n",
    "# generalized pareto    \n",
    "power_l=0.9;\n",
    "power_s= 0.8;  \n",
    "eta=250;\n",
    "k= -0.1;\n",
    "theta= 14;\n",
    "\n",
    "#parameters for cost definition\n",
    "beta=14 ; #coefficient of residents' investment decision y_n, residents', contribution of 14M $/yearly, 1M $ taken as unit\n",
    "alpha=300; # 25m $ is the coefficient for investment cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-135d214c58df>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mQ_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost_array\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_array\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQ_target\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\icml_code\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\icml_code\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\icml_code\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\icml_code\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\icml_code\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a_g= 0.9; ## Government's cooperation index\n",
    "a_r= 0.9; ## Residents' cooperation index\n",
    "a=2; ## SLR scenarios; a=0,1, and 2, respectively for intermediate low, intermediate, and high sea level rise projections. \n",
    "s_norm=400,# normalozing s\n",
    "l_norm=[580, 1190,2590];# normalozing s\n",
    "scene= ['low','inter','high']\n",
    "\n",
    "EPISODES= 1000000;\n",
    "Final_Episodes=100; ## episodes used for final cost evaluation\n",
    "\n",
    "years=100; #future to be considered in years \n",
    "\n",
    "agent = DQNAgent(a_g);\n",
    "#agent.load(\"./slr_dqn_{}.h5\".format(scene[a]))                   \n",
    "\n",
    "batch_size=1000;\n",
    "state= np.zeros((100,2));\n",
    "next_state= np.zeros((100,2));\n",
    "convergence=[];\n",
    "w00= np.zeros((2,12));\n",
    "w01= np.zeros((12,24));\n",
    "w02= np.zeros((24,12));\n",
    "w03= np.zeros((12,5));\n",
    "b00= np.zeros(12);\n",
    "b01= np.zeros(24);\n",
    "b02= np.zeros(12);\n",
    "b03= np.zeros(5);\n",
    "\n",
    "total_cost= 0; ## average cost over all episodes\n",
    "final_cost= 0; ## average cost for last 10000 episodes(convergence expected by then)\n",
    "\n",
    "for e in range(EPISODES):\n",
    "\tl= np.zeros(101);\n",
    "\ts= np.zeros(101);\n",
    "\tl[0]=100; # initial relative sea level\n",
    "\ts[0]=50; # initial infrastructure state  \n",
    "\tr=0; # sea level rise each year\n",
    "\tq= np.zeros(101);  ## yearly residents' decision score          \n",
    "\tp= np.zeros(101);  ## yearly sigmoid input for residents' decision           \n",
    "\tres= np.zeros(101); ## yearly residents' binary decision \n",
    "\tx= np.zeros(100);   ##  yearly govenment's decision         \n",
    "\tz= np.zeros(100); ## yearly cost from nature\n",
    "            \n",
    "\tfor y in range(years):\n",
    "\n",
    "\t\tz[y]=genpareto.rvs(k, loc=theta, scale=eta* np.power(l[y],power_l)/np.power(s[y],power_s)); # genpareto, shape,k=-0.001,location,theta=0; scale,sigma         \n",
    "               \n",
    "\t\tstate[y,0]= (l[y]-l[0])/l_norm[a];             \n",
    "\t\tstate[y,1]= (s[y]-s[0])/s_norm;             \n",
    "                \n",
    "\t\taction = agent.act(np.reshape(state[y],(1,2)));\n",
    "\t\tx[y]= action; \n",
    "\t\tnext_state[y,0]= (l[y]-l[0]+A[a,y]/2)/l_norm[a]; #  As the growth rate of shape parameter is fixed for any scenario,so {A[a,y]/2+A[a,6]/2-A[a,5]/2} simply means expected sea level rise for next year            \n",
    "\t\tnext_state[y,1]= (s[y]-s[0]+action)/s_norm;  \n",
    "\t\tcost= alpha*x[y]-beta*res[y]+z[y];  \n",
    "\t\ttotal_cost=total_cost+ cost;\n",
    "\t\tif e> np.absolute(EPISODES-Final_Episodes-1):\n",
    "\t\t\tfinal_cost=final_cost+ cost;                        \n",
    "\t\tagent.remember(state[y], action, cost, next_state[y]) # normalizing reward by guessing mean 1500 and max cost 12936(l=2600mm,s=50)                              \n",
    "\t\tif len(agent.memory) > batch_size:\n",
    "\t\t\tw0= agent.model.layers[0].get_weights()[0];\n",
    "\t\t\tw1= agent.model.layers[1].get_weights()[0];                        \n",
    "\t\t\tw2= agent.model.layers[2].get_weights()[0];                        \n",
    "\t\t\tw3= agent.model.layers[3].get_weights()[0];  \n",
    "\t\t\tb0= agent.model.layers[0].get_weights()[1];\n",
    "\t\t\tb1= agent.model.layers[1].get_weights()[1];                        \n",
    "\t\t\tb2= agent.model.layers[2].get_weights()[1];                        \n",
    "\t\t\tb3= agent.model.layers[3].get_weights()[1];                        \n",
    "\t\t\tconv=np.sum(np.power((w0-w00),2))+np.sum(np.power((w1-w01),2))+np.sum(np.power((w2-w02),2))+np.sum(np.power((w3-w03),2))+np.sum(np.power((b0-b00),2))+np.sum(np.power((b1-b01),2))+np.sum(np.power((b2-b02),2))+np.sum(np.power((b3-b03),2));\n",
    "\t\t\tconvergence=  np.append(convergence,conv);\n",
    "\t\t\tw00=w0;\n",
    "\t\t\tw01=w1;\n",
    "\t\t\tw02=w2;\n",
    "\t\t\tw03=w3;\n",
    "\t\t\tb00=b0;\n",
    "\t\t\tb01=b1;\n",
    "\t\t\tb02=b2;\n",
    "\t\t\tb03=b3;                        \n",
    "\t\t\tagent.replay(batch_size); \n",
    "\t\t\tif y % 30 == 1:       \n",
    "\t\t\t\tagent.target_train() \n",
    "                \n",
    "\t\tq[y+1]=a_r*(q[y]+ action/4 * z[y]); ## dividing the action by 4 because action can be o~3\n",
    "\t\tp[y+1]= 1/(1 + np.exp(-(q[y+1]-5)));\n",
    "\t\tres[y+1]= np.random.binomial(1, p[y+1]);  \n",
    "\n",
    "\t\tr= np.random.gamma(A[a,y],B);                                         \n",
    "\t\tl[y+1]= l[y]+r;                \n",
    "\t\ts[y+1]= s[y]+action; \n",
    "        \n",
    "\n",
    "#agent.save(\"./slr_dqn_{}.h5\".format(scene[a]))                 \n",
    "            \n",
    "total_cost=total_cost/EPISODES;\n",
    "final_cost=final_cost/Final_Episodes;\n",
    "print(\"Total cost: {}, final cost: {}\" .format(total_cost,final_cost))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
