{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import random\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import genpareto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Critic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, learning_rate=3e-4):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.num_inputs=2 \n",
    "        self.num_actions = 5\n",
    "        self.critic_linear1 = nn.Linear(self.num_inputs, 12)\n",
    "        self.critic_linear2 = nn.Linear(12, 120)\n",
    "        self.critic_linear3 = nn.Linear(120, 12)        \n",
    "        self.critic_linear4 = nn.Linear(12, 1)\n",
    "\n",
    "        self.actor_linear1 = nn.Linear(self.num_inputs, 12)\n",
    "        self.actor_linear2 = nn.Linear(12, 12)\n",
    "        #self.actor_linear3 = nn.Linear(120, 12)\n",
    "        self.actor_linear4 = nn.Linear(12, self.num_actions)\n",
    "   \n",
    "    def forward(self, state):\n",
    "        state = Variable(torch.from_numpy(state).float())\n",
    "        value = F.relu(self.critic_linear1(state))\n",
    "        value = F.relu(self.critic_linear2(value))\n",
    "        value = F.relu(self.critic_linear3(value))                \n",
    "        value = self.critic_linear4(value)\n",
    "        \n",
    "        policy_dist = F.relu(self.actor_linear1(state))\n",
    "        policy_dist = F.relu(self.actor_linear2(policy_dist))\n",
    "        #policy_dist = F.relu(self.actor_linear3(policy_dist))                \n",
    "        policy_dist = F.softmax(self.actor_linear4(policy_dist), dim=1)\n",
    "\n",
    "        return value, policy_dist      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOAA sea level rise predictions\n",
    "#input 'scale_parameter_gammadstrb'\n",
    "B= 0.5;\n",
    "\n",
    "#input 'shape_parameter_gammadstrb'\n",
    "A= np.zeros((3,100))\n",
    "A[0,:]=np.arange(11.2,12.388,0.012);# intermediate low rise case\n",
    "A[1,:]=np.arange(13,35,0.22);# intermediate high rise case\n",
    "A[2,:]= np.arange(14.6,88.6,0.74);# high rise case\n",
    "\n",
    "\n",
    "# generalized pareto    \n",
    "power_l=0.9;\n",
    "power_s= 0.8;  \n",
    "eta=250;\n",
    "k= -0.1;\n",
    "theta= 14;\n",
    "\n",
    "#parameters for cost definition\n",
    "beta=14 ; #coefficient of residents' investment decision y_n, residents', contribution of 14M $/yearly, 1M $ taken as unit\n",
    "alpha=300; # 25m $ is the coefficient for investment cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "GAMMA = 0.99\n",
    "a_g= 0.9; ## Government's cooperation index\n",
    "a_r= 0.9; ## Residents' cooperation index\n",
    "a=2; ## SLR scenarios; a=0,1, and 2, respectively for intermediate low, intermediate, and high sea level rise projections. \n",
    "\n",
    "s_norm=400,# normalozing s\n",
    "l_norm=[580, 1190,2590];# normalozing s\n",
    "years = 100\n",
    "episodes = 1000000\n",
    "\n",
    "actor_critic = ActorCritic()\n",
    "ac_optimizer = optim.Adam(actor_critic.parameters(), lr=3e-4)\n",
    "\n",
    "all_rewards = []\n",
    "entropy_term = 0\n",
    "    \n",
    "for episode in range(episodes):\n",
    "\tlog_probs = []; # log probabilities of action\n",
    "\tvalues = np.zeros(years)\n",
    "\trewards = np.zeros(years)\n",
    "\n",
    "\n",
    "\tl= np.zeros(101);\n",
    "\ts= np.zeros(101);\n",
    "\tl[0]=100; # initial relative sea level\n",
    "\ts[0]=50; # initial infrastructure state  \n",
    "\tr=0; # sea level rise each year\n",
    "\tq= np.zeros(101);  # yearly residents' decision score          \n",
    "\tsig= np.zeros(101);# yearly sigmoid input for residents' decision           \n",
    "\tres= np.zeros(101); # yearly residents' binary decision \n",
    "\tx= np.zeros(100); #  yearly govenment's decision         \n",
    "\tz= np.zeros(100); # yearly cost from nature\n",
    "\tstate= np.zeros((1,2));\n",
    "\tnext_state= np.zeros((1,2));\n",
    "\tfor y in range(years):\n",
    "\t\tstate[0,0]= (l[y]-l[0])/l_norm[a];             \n",
    "\t\tstate[0,1]= (s[y]-s[0])/s_norm;             \n",
    "\n",
    "\t\tvalue, policy_dist = actor_critic.forward(state)\n",
    "\t\tvalues[y] = value.detach().numpy()# converts tensor to array\n",
    "\t\tdist = policy_dist.detach().numpy() # converts tensor to array\n",
    "\n",
    "\t\taction =np.random.choice(5, p=np.squeeze(dist)) # random.randrange(5)\n",
    "\t\tx[y]= action;\n",
    "\t\tz[y]=genpareto.rvs(k, loc=theta, scale=eta* np.power(l[y],power_l)/np.power(s[y],power_s)); # genpareto, shape,k=-0.1,location,theta=0; scale,sigma                    \n",
    "\t\tq[y+1]=a_r*(q[y]+ action/4 * z[y]); ## dividing the action by 4 because action can be o~3\n",
    "\t\tsig[y+1]= 1/(1 + np.exp(-(q[y+1]-5)));\n",
    "\t\tres[y+1]= np.random.binomial(1, sig[y+1]);  \n",
    "\n",
    "\t\tr= np.random.gamma(A[a,y],B);                                         \n",
    "\t\tl[y+1]= l[y]+r;                \n",
    "\t\ts[y+1]= s[y]+action; \n",
    "\t\tnext_state[0,0]= (l[y+1]-l[0])/l_norm[a];            \n",
    "\t\tnext_state[0,1]= (s[y+1]-s[0])/s_norm;  \n",
    "\t\trewards[y]=alpha*x[y]-beta*res[y]+z[y];\n",
    "        \n",
    "\t\tlog_prob = torch.log(policy_dist.squeeze(0)[action])\n",
    "\t\tentropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "\t\tlog_probs.append(log_prob)\n",
    "\t\tentropy_term += entropy\n",
    "\t\tstate = next_state\n",
    "            \n",
    "\tQval, transition = actor_critic.forward(next_state)\n",
    "\tQval = Qval.detach().numpy()\n",
    "\tall_rewards.append(np.sum(rewards))\n",
    "            \n",
    "        # compute Q values\n",
    "\tQvals = np.zeros(years)\n",
    "\tfor t in reversed(range(years)):\n",
    "\t\tQval = -rewards[t] + GAMMA * Qval\n",
    "\t\tQvals[t] = Qval\n",
    "  \n",
    "        #update actor critic\n",
    "\tvalues = torch.FloatTensor(values)\n",
    "\tQvals = torch.FloatTensor(Qvals)\n",
    "\tlog_probs = torch.stack(log_probs)\n",
    "        \n",
    "\tadvantage = Qvals - values\n",
    "\tactor_loss = (-log_probs * advantage).mean()\n",
    "\tcritic_loss = 0.5 * advantage.pow(2).mean()\n",
    "\tac_loss = actor_loss + critic_loss + 0.001 * entropy_term\n",
    "\n",
    "\tac_optimizer.zero_grad() #Sets gradients of all model parameters to zero, needs to be done before \"ac_loss.backward()\"\n",
    "\tac_loss.backward()\n",
    "\tac_optimizer.step()\n",
    "    \n",
    "    # Plot results\n",
    "smoothed_rewards = pd.Series.rolling(pd.Series(all_rewards), 10).mean()\n",
    "smoothed_rewards = [elem for elem in smoothed_rewards]\n",
    "plt.plot(all_rewards)\n",
    "plt.plot(smoothed_rewards)\n",
    "plt.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Cost in M$')\n",
    "plt.show()\n",
    "\n",
    "print(\"Total cost: {}, final cost: {}\" .format(np.sum(all_rewards)/episodes,np.sum(all_rewards[(episodes-10000):episodes])/10000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
